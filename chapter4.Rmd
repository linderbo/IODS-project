# Excercise 4 - Clustering and classification

```{r 4setup, include = FALSE}
library(dplyr)
library(ggplot2)
library(GGally)
library(corrplot)
```

## Introduction and data

We use the built-in R `Boston` data set which explores housing values in suburbs of Boston.

```{r 4loaddata}
# load the library MASS, and the data.
library(MASS)
data(Boston)

str(Boston)
dim(Boston)
```
An explanation of the 14 variables can be found [here.](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html) 12 are numeric, `chas` is a binary masked as an integer (0 or 1) and `rad` is an index in integer form (`r min(Boston$rad) ` to `r max(Boston$rad) `). There are `r nrow(Boston)` observations of each variable.

```{r 4pairs, fig.width = 10, fig.height=6}

par <- ggpairs(Boston, mapping = aes(alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
par
```

The `ggpairs()` plot contains a lot of information, and is a bit crowded with so many variables althoug you can see some patterns appearing with regards to correlation. Most variables do not seem to be normally distributed. Lets have a look at a colored correlation plot.

```{r 4corrplot}
# assign correlation matrix
cor_mtrx <- cor(Boston) 
# plot 
corrplot(cor_mtrx, method="circle", type="lower", cl.pos="b", tl.pos="d", tl.cex=0.7)
```

This nicely shows correlation between variables. To name a few: `indus` (proportion of land in use of industry) and `nox` (concentration of nitrogen oxides) are positively correlated, and negatively correlated with `dis` (distance to highways). This seems reasonable.

## We´ll standardise the data
```{r 4standardize}
# center and standardize variables
boston_scaled <- scale(Boston, center=T, scale=T)

# change matrix to data frame
boston_scaled <- as.data.frame(boston_scaled)

# summaries of the scaled variables
summary(boston_scaled)

```

With ´scale()` on default settings we center the data on 0, and divide each variable by its standard deviation, effectively making the SD = 1 for each variable. We do this due to the requirements of *Linear discriminant analysis*.

## Some data wrangling

Now we create a categorical variable for crime rate from the quantiles of `crim`. Also we divide the data into a `train` and `test` set with 80% of data in the train set. Well remove the correct `crime` classes from the test set after saving them separately as `true_crime`.

```{r 4categorize crime}

# create a quantile vector of crim 
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime' with the quantiles.
lblz <- c("low", "med_low", "med_high","high")
crime <- cut(boston_scaled$crim, breaks = bins, labels = lblz, include.lowest = TRUE)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# randomly choose 80% of rows, yields 404 rows.
selection <- sample(nrow(boston_scaled), size = 0.8*nrow(boston_scaled))

#gen train set
train <- boston_scaled[selection,]

#gen test set
test <- boston_scaled[-selection,]

# remove correct classes from test set.
true_crime <- test$crime
test <- dplyr::select(test, -crime)
```

## After this data wrangling we are ready for linear discriminant analysis with `crime` as target variable, and all others as predictors.

```{r, fig.width = 10, fig.height=6}
#we fit the modelwith crime
lda.fit <- lda(crime ~ ., data = train)

# we define a function for producing arrows in the plot
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "magenta", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

#
crime_classes <- as.numeric(train$crime) #for colors in plot
plot(lda.fit, col = crime_classes, pch = crime_classes, dimen =2)
lda.arrows(lda.fit, myscale =2)

#plot(boston_scaled$crime, boston_scaled$rad)
```

Our first linear discriminant LD1, which is on the x-axis separates the `high` from `low` to `med_high` classes very well, whereas LD2 (y axis) does not do a too good job on this, slthoug separation between `low` and `med_high` is apparent. I guess that our model would be good at predicting wether crime rate is exceptionally high. The predictor with most influence is `rad` (accessibillity to radial highways), followedby `zn` and `nox`.

## We´ll predict classes in the test dataset.
```{r 4predict}
# did the removing of crime from test set already in the chunk "categorize crime"
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = true_crime, predicted = lda.pred$class)

```

The model is able to predict crime rate class pretty well. It has most trouble separating `med_low` from its neighbors.

## And now for some distance measures
We´ll count euclidean distances for the standardised and centered Boston dataset and view a summary of distances. Then well run kmeans on the data with 4 clusters, and have a look at an example plot with some of the variables.

```{r 4distances and k-means}
data(Boston)
centbost <- scale(Boston, center=T, scale=T)
dist_eu <- dist(centbost)
summary(dist_eu)

#run kmeans
km <- kmeans(centbost, centers = 4)

#plot
pairs(Boston[c(1,3,5,8,9)], col = km$cluster)

```

We´ll calculate the otpimum number of clusters by looking at within cluster sum of squares based on how many clusters we use. We plot wcss by number of clusters, and the optimum number of clusters should be where wcss drops radically.  

```{r 4 clusternumber}

# calculate the total within sum of squares
twcss <- sapply(1:10, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:10, y = twcss, geom = 'line')
```

Two in this case.
Well run the kmeans with 2 centers, and plot the pairs in Boston coloring by these centers.

```{r 4 2clusters, fig.width = 10, fig.height=6}
# k-means clustering
km2 <-kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km2$cluster)

```

This doesn´t look too exciting. The clustering doesn´t produce distinct separation on any pairs of variables.

## Let´s try the Bonus excercise:

```{r 4bonus1, fig.width = 10, fig.height=6}

data(Boston)
BBoston <- as.data.frame(scale(Boston, center=T, scale=T)) # center and scale, as data frame


#run kmeans
kmB <- kmeans(BBoston, centers = 3)

#we fit the modelwith crime
lda.fit <- lda(kmB$cluster ~ ., data = BBoston)

# we define a function for producing arrows in the plot
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "magenta", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

#
# crime_classes <- as.numeric(train$crime) #for colors in plot
plot(lda.fit, col = kmB$cluster, dimen =2)
lda.arrows(lda.fit, myscale =2)

```

The most influential separators for our clusters are `rad`, `tax` and `age`